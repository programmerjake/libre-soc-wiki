# SimdShape

A logical extension of the nmigen `ast.Shape` concept, `SimdShape`
provides sufficient context to both define overrides for individual lengths
on a per-mask basis as well as sufficient information to "upcast"
back to a SimdSignal, in exactly the same way that c++ virtual base
class upcasting works when RTTI (Run Time Type Information) works.

By deriving from `ast.Shape` both `width` and `signed` are provided
already, leaving the `SimdShape` class with the responsibility to
additionally define lengths for each mask basis. This is best illustrated
with an example.

The Libre-SOC IEEE754 ALUs need to be converted to SIMD Partitioning
but without massive disruptive code-duplication or intrusive explicit
coding as outlined in the worst of the techniques documented in
[[dynamic_simd]].  This in turn implies that Signals need to be declared
for both mantissa and exponent that **change width to non-power-of-two
sizes** depending on Partition Mask Context.

Mantissa:

* when the context is 1xFP64 the mantissa is 54 bits (excluding guard
  rounding and sticky)
* when the context is 2xFP32 there are **two** mantissas of 23 bits
* when the context is 4xFP16 there are **four** mantissas of 10 bits
* when the context is 4xBF16 there are four mantissas of 5 bits.

Exponent:

* 1xFP64: 11 bits, one exponent
* 2xFP32: 8 bits, two exponents
* 4xFP16: 5 bits, four exponents
* 4xBF16: 8 bits, four exponents

`SimdShape` needs this information in addition to the normal
information (width, sign) in order to create the partitions
that allow standard nmigen operations to **transparently**
and naturally take place at **all** of these non-uniform
widths, as if they were in fact scalar Signals *at* those
widths.

A minor wrinkle which emerges from deep analysis is that the overall
available width (`Shape.width`) does in fact need to be explicitly
declared, and
the sub-partitions to fit onto power-of-two boundaries, in order to allow
straight wire-connections rather than allow the SimdSignal to be
arbitrary-sized (compact).  Although on shallow inspection this
initially would seem to imply that it would result in large unused
sub-partitions (padding partitions) these gates can in fact be eliminated
with a "blanking" mask, created from static analysis of the SimdShape
context.

Example:

* all 32 and 16-bit values are actually to be truncated to 11 bit
* all 8-bit values to 5-bit

from these we can write out the allocations, bearing in mind that
in each partition the sub-signal must start on a power-2 boundary,
and that "x" marks unused (padding) portions:
 
          |31|  |  |  |     16|15|  |   8|7     0 |
    32bit | x| x| x|  |      x| x| x|10 ....    0 |
    16bit | x| x|26    ... 16 | x| x|10 ....    0 |
    8bit  | x|28 .. 24|  20.16| x|12 .. 8|x|4.. 0 |

thus, we deduce, we *actually* need breakpoints at these positions,
and that unused portions common to **all** cases can be deduced
and marked "x" by looking at the columns above which all contain "x":

          |  |28|26|24| |20|16|  |12|10|8| |4     |
            x                   x

These 100% unused "x"s therefore define the "blanking" mask, and in
these sub-portions it is unnecessary to allocate computational gates.

Also in order to save gates, in the example above there are only three
cases (32 bit, 16 bit, 8 bit) therefore only three sets of logic
are required to construct the larger overall computational result
from the "smaller chunks". At first glance, with there
being 9 actual partitions (28, 26, 24, 20, 16, 12, 10, 8, 4), it
would appear that 2^9 (512!) cases were required, where in fact
there are only three.

These facts also need to be communicated to both the SimdSignal
as well as the submodules implementing its core functionality:
add operation and other arithmetic behaviour, as well as
[[dynamic_simd/cat]] and others.

In addition to that, there is a "convenience" that emerged
from technical discussions as desirable
to have, which is that it should be possible to perform
rudimentary arithmetic operations *on a SimdShape* which preserves
or adapts the Partition context, where the arithmetic operations
occur on `Shape.width`.

    >>> XLEN = SimdShape(64, signed=True, ...)
    >>> x2 = XLEN // 2
    >>> print(x2.width)
    32
    >>> print(x2.signed)
    True

With this capability it becomes possible to use the Liskov Substitution
Principle in dynamically compiling code that switches between scalar and
SIMD transparently:

    # scalar context
    scalarctx = scl = object()
    scl.XLEN = 64
    scl.SigKls = Signal         # standard nmigen Signal
    # SIMD context
    simdctx = sdc = object()
    sdc = SimdShape(64, ....)
    sdc.SigKls = SimdSignal     # advanced SIMD Signal
    sdc.elwidth = Signal(2)

    # select one 
    if compiletime_switch == 'SIMD':
        ctx = simdctx
    else:
        ctx = scalarctx

    # exact same code switching context at compile time
    m = Module():
    with ctx:
        x = ctx.SigKls(ctx.XLEN)
        ...
    m.d.comb += x.eq(Const(3))

